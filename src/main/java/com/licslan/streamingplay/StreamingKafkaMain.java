package com.licslan.streamingplay;

import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.common.serialization.StringDeserializer;
import org.apache.spark.SparkConf;
import org.apache.spark.TaskContext;
import org.apache.spark.streaming.Durations;
import org.apache.spark.streaming.api.java.JavaInputDStream;
import org.apache.spark.streaming.api.java.JavaPairDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;
import org.apache.spark.streaming.kafka010.*;
import scala.Tuple2;

import java.util.Arrays;
import java.util.Collection;
import java.util.HashMap;
import java.util.Map;
/**
 * @author  Weilin Huang
 * 从kafka读取内容  并进行相关数据处理操作 demo
 *
 * 提交方式：
 *
 * spark-submit \
 *         --master local[*] \  //no hard code in your code
 *         --class com.icslan.streamingplay.StreamingKafkaMain \
 *         --executor-memory 4g \
 *         --executor-cores 4 \
 *         /linux path of jars/scala-1.0-SNAPSHOT.jar
 *
 * */
public class StreamingKafkaMain {


    public static void main(String[] args) throws Exception{

        /**加入kafka 依赖
         *
         * Basic Concepts
         * Next, we move beyond the simple example and elaborate on the basics of Spark Streaming.
         *
         * Linking
         * Similar to Spark, Spark Streaming is available through Maven Central. To write your own
         * Spark Streaming program, you will have to add the following dependency to your SBT or Maven project.
         *
         * <dependency>
         *     <groupId>org.apache.spark</groupId>
         *     <artifactId>spark-streaming_2.11</artifactId>
         *     <version>2.3.2</version>
         * </dependency>
         * */


        /**
         * Initializing StreamingContext
         * To initialize a Spark Streaming program, a StreamingContext object has to be created
         * which is the main entry point of all Spark Streaming functionality.
         * */


        SparkConf conf = new SparkConf()
               /* .setMaster("local[*]")*/
                .setAppName("StreamingKafka");
        JavaStreamingContext javaStreamingContext = new JavaStreamingContext(conf, Durations.seconds(2));


        /**
         * After a context is defined, you have to do the following.
         *
         * Define the input sources by creating input DStreams.
         * Define the streaming computations by applying transformation and output operations to DStreams.
         * Start receiving data and processing it using streamingContext.start().
         * Wait for the processing to be stopped (manually or due to any error) using streamingContext.awaitTermination().
         * The processing can be manually stopped using streamingContext.stop().
         * */

        //下面的几点需要注意！！！
        /**
         * Points to remember:
         * Once a context has been started, no new streaming computations can be set up or added to it.
         * Once a context has been stopped, it cannot be restarted.
         * Only one StreamingContext can be active in a JVM at the same time.
         * stop() on StreamingContext also stops the SparkContext. To stop only the StreamingContext, set the optional parameter of stop() called stopSparkContext to false.
         * A SparkContext can be re-used to create multiple StreamingContexts, as long as the previous StreamingContext is stopped (without stopping the SparkContext) before the next StreamingContext is created.
         * */


        /**
         * Discretized Stream or DStream is the basic abstraction provided by Spark Streaming. It represents
         * a continuous stream of data, either the input data stream received from source, or the processed
         * data stream generated by transforming the input stream. Internally, a DStream is represented by
         * a continuous series of RDDs, which is Spark’s abstraction of an immutable, distributed dataset
         * (see Spark Programming Guide for more details). Each RDD in a DStream contains data from a certain
         * interval, as shown in the following figure.
         *
         *Any operation applied on a DStream translates to operations on the underlying RDDs. For example,
         * in the earlier example of converting a stream of lines to words, the flatMap operation is applied
         * on each RDD in the lines DStream to generate the RDDs of the words DStream. This is shown in the following figure.
         *
         * These underlying RDD transformations are computed by the Spark engine. The DStream operations hide most of these
         * details and provide the developer with a higher-level API for convenience. These operations are discussed in detail in later sections.
         *
         *Input DStreams and Receivers
         * Input DStreams are DStreams representing the stream of input data received from streaming sources. In the quick example, lines was an input DStream as it represented the stream of data received from
         * the netcat server. Every input DStream (except file stream, discussed later in this section) is associated with a Receiver (Scala doc, Java doc) object which receives the data from a source and stores
         * it in Spark’s memory for processing.
         *
         * Spark Streaming provides two categories of built-in streaming sources.
         *
         * Basic sources: Sources directly available in the StreamingContext API. Examples: file systems, and socket connections.
         * Advanced sources: Sources like Kafka, Flume, Kinesis, etc. are available through extra utility classes. These require linking against extra dependencies as discussed in the linking section.
         * We are going to discuss some of the sources present in each category later in this section.
         *
         * Note that, if you want to receive multiple streams of data in parallel in your streaming application, you can create multiple input DStreams (discussed further in the Performance Tuning section).
         * This will create multiple receivers which will simultaneously receive multiple data streams. But note that a Spark worker/executor is a long-running task, hence it occupies one of the cores allocated
         * to the Spark Streaming application. Therefore, it is important to remember that a Spark Streaming application needs to be allocated enough cores (or threads, if running locally) to process the received
         * data, as well as to run the receiver(s).
         *
         * Points to remember
         * When running a Spark Streaming program locally, do not use “local” or “local[1]” as the master URL. Either of these means that only one thread will be used for running tasks locally. If you are using
         * an input DStream based on a receiver (e.g. sockets, Kafka, Flume, etc.), then the single thread will be used to run the receiver, leaving no thread for processing the received data. Hence, when running
         * locally, always use “local[n]” as the master URL, where n > number of receivers to run (see Spark Properties for information on how to set the master).
         *
         * Extending the logic to running on a cluster, the number of cores allocated to the Spark Streaming application must be more than the number of receivers. Otherwise the system will receive data, but not
         * be able to process it.
         *
         * Basic Sources
         * We have already taken a look at the ssc.socketTextStream(...) in the quick example which creates a DStream from text data received over a TCP socket connection. Besides sockets, the StreamingContext
         * API provides methods for creating DStreams from files as input sources.
         *
         * File Streams
         * For reading data from files on any file system compatible with the HDFS API (that is, HDFS, S3, NFS, etc.), a DStream can be created as via StreamingContext.fileStream[KeyClass, ValueClass, InputFormatClass].
         *
         * File streams do not require running a receiver so there is no need to allocate any cores for receiving file data.
         *
         * For simple text files, the easiest method is StreamingContext.textFileStream(dataDirectory).
         *
         * plz access website http://spark.apache.org/docs/latest/streaming-programming-guide.html#initializing-streamingcontext
         * */

        //javaStreamingContext.fileStream<KeyClass, ValueClass, InputFormatClass>(dataDirectory);
        javaStreamingContext.textFileStream("/path of dataDirectory");

        /**
         * How Directories are Monitored
         * Spark Streaming will monitor the directory dataDirectory and process any files created in that directory.
         *
         * A simple directory can be monitored, such as "hdfs://namenode:8040/logs/". All files directly under such a path will be processed as they are discovered.
         * A POSIX glob pattern can be supplied, such as "hdfs://namenode:8040/logs/2017/*". Here, the DStream will consist of all files in the directories matching the pattern. That is: it is a pattern of
         * directories, not of files in directories.
         * All files must be in the same data format.
         * A file is considered part of a time period based on its modification time, not its creation time.
         * Once processed, changes to a file within the current window will not cause the file to be reread. That is: updates are ignored.
         * The more files under a directory, the longer it will take to scan for changes — even if no files have been modified.
         * If a wildcard is used to identify directories, such as "hdfs://namenode:8040/logs/2016-*", renaming an entire directory to match the path will add the directory to the list of monitored directories.
         * Only the files in the directory whose modification time is within the current window will be included in the stream.
         * Calling FileSystem.setTimes() to fix the timestamp is a way to have the file picked up in a later window, even if its contents have not changed.
         * Using Object Stores as a source of data
         * “Full” Filesystems such as HDFS tend to set the modification time on their files as soon as the output stream is created. When a file is opened, even before data has been completely written,
         * it may be included in the DStream - after which updates to the file within the same window will be ignored. That is: changes may be missed, and data omitted from the stream.
         *
         * To guarantee that changes are picked up in a window, write the file to an unmonitored directory, then, immediately after the output stream is closed, rename it into the destination directory.
         * Provided the renamed file appears in the scanned destination directory during the window of its creation, the new data will be picked up.
         *
         * In contrast, Object Stores such as Amazon S3 and Azure Storage usually have slow rename operations, as the data is actually copied. Furthermore, renamed object may have the time of the rename()
         * operation as its modification time, so may not be considered part of the window which the original create time implied they were.
         *
         * Careful testing is needed against the target object store to verify that the timestamp behavior of the store is consistent with that expected by Spark Streaming. It may be that writing directly
         * into a destination directory is the appropriate strategy for streaming data via the chosen object store.
         *
         * For more details on this topic, consult the Hadoop Filesystem Specification.
         *
         * Streams based on Custom Receivers
         * DStreams can be created with data streams received through custom receivers. See the Custom Receiver Guide for more details.
         *
         * Queue of RDDs as a Stream
         * For testing a Spark Streaming application with test data, one can also create a DStream based on a queue of RDDs, using streamingContext.queueStream(queueOfRDDs). Each RDD pushed into the queue
         * will be treated as a batch of data in the DStream, and processed like a stream.
         *
         * For more details on streams from sockets and files, see the API documentations of the relevant functions in StreamingContext for Scala, JavaStreamingContext for Java, and StreamingContext for Python.
         *
         *
         *
         *
         *
         * Advanced Sources
         * Python API As of Spark 2.3.2, out of these sources, Kafka, Kinesis and Flume are available in the Python API.
         *
         * This category of sources require interfacing with external non-Spark libraries, some of them with complex dependencies (e.g., Kafka and Flume). Hence, to minimize issues related to version
         * conflicts of dependencies, the functionality to create DStreams from these sources has been moved to separate libraries that can be linked to explicitly when necessary.
         *
         * Note that these advanced sources are not available in the Spark shell, hence applications based on these advanced sources cannot be tested in the shell. If you really want to use them in the
         * Spark shell you will have to download the corresponding Maven artifact’s JAR along with its dependencies and add it to the classpath.
         *
         * Some of these advanced sources are as follows.
         *
         * Kafka: Spark Streaming 2.3.2 is compatible with Kafka broker versions 0.8.2.1 or higher. See the Kafka Integration Guide for more details.
         *
         * Flume: Spark Streaming 2.3.2 is compatible with Flume 1.6.0. See the Flume Integration Guide for more details.
         *
         * Kinesis: Spark Streaming 2.3.2 is compatible with Kinesis Client Library 1.2.1. See the Kinesis Integration Guide for more details.
         *
         * Custom Sources
         * Python API This is not yet supported in Python.
         *
         * Input DStreams can also be created out of custom data sources. All you have to do is implement a user-defined receiver (see next section to understand what that is) that can receive data
         * from the custom sources and push it into Spark. See the Custom Receiver Guide for details.
         *
         * Receiver Reliability
         * There can be two kinds of data sources based on their reliability. Sources (like Kafka and Flume) allow the transferred data to be acknowledged. If the system receiving data from these
         * reliable sources acknowledges the received data correctly, it can be ensured that no data will be lost due to any kind of failure. This leads to two kinds of receivers:
         *
         * Reliable Receiver - A reliable receiver correctly sends acknowledgment to a reliable source when the data has been received and stored in Spark with replication.
         * Unreliable Receiver - An unreliable receiver does not send acknowledgment to a source. This can be used for sources that do not support acknowledgment, or even for reliable sources when
         * one does not want or need to go into the complexity of acknowledgment.
         * The details of how to write a reliable receiver are discussed in the Custom Receiver Guide.
         * */

        //转换算子  //行动算子  //控制算子
        /**
         * Transformations on DStreams
         * Similar to that of RDDs, transformations allow the data from the input DStream to be modified. DStreams support many of the transformations available on normal Spark RDD’s. Some of the
         * common ones are as follows.
         *
         * Transformation	Meaning
         * map(func)	Return a new DStream by passing each element of the source DStream through a function func.
         * flatMap(func)	Similar to map, but each input item can be mapped to 0 or more output items.
         * filter(func)	Return a new DStream by selecting only the records of the source DStream on which func returns true.
         * repartition(numPartitions)	Changes the level of parallelism in this DStream by creating more or fewer partitions.
         * union(otherStream)	Return a new DStream that contains the union of the elements in the source DStream and otherDStream.
         * count()	Return a new DStream of single-element RDDs by counting the number of elements in each RDD of the source DStream.
         * reduce(func)	Return a new DStream of single-element RDDs by aggregating the elements in each RDD of the source DStream using a function func (which takes two arguments and returns one). The
         * function should be associative and commutative so that it can be computed in parallel.
         * countByValue()	When called on a DStream of elements of type K, return a new DStream of (K, Long) pairs where the value of each key is its frequency in each RDD of the source DStream.
         * reduceByKey(func, [numTasks])	When called on a DStream of (K, V) pairs, return a new DStream of (K, V) pairs where the values for each key are aggregated using the given reduce function.
         * Note: By default, this uses Spark's default number of parallel tasks (2 for local mode, and in cluster mode the number is determined by the config property spark.default.parallelism) to do
         * the grouping. You can pass an optional numTasks argument to set a different number of tasks.
         * join(otherStream, [numTasks])	When called on two DStreams of (K, V) and (K, W) pairs, return a new DStream of (K, (V, W)) pairs with all pairs of elements for each key.
         * cogroup(otherStream, [numTasks])	When called on a DStream of (K, V) and (K, W) pairs, return a new DStream of (K, Seq[V], Seq[W]) tuples.
         * transform(func)	Return a new DStream by applying a RDD-to-RDD function to every RDD of the source DStream. This can be used to do arbitrary RDD operations on the DStream.
         * updateStateByKey(func)	Return a new "state" DStream where the state for each key is updated by applying the given function on the previous state of the key and the new values for the key.
         * This can be used to maintain arbitrary state data for each key.
         * */






        /**
         * Output operations allow DStream’s data to be pushed out to external systems like a database or a file systems. Since the output operations actually allow the transformed data to be consumed
         * by external systems, they trigger the actual execution of all the DStream transformations (similar to actions for RDDs). Currently, the following output operations are defined:
         *
         * Output Operation	Meaning
         * print()	Prints the first ten elements of every batch of data in a DStream on the driver node running the streaming application. This is useful for development and debugging.
         * Python API This is called pprint() in the Python API.
         * saveAsTextFiles(prefix, [suffix])	Save this DStream's contents as text files. The file name at each batch interval is generated based on prefix and suffix: "prefix-TIME_IN_MS[.suffix]".
         * saveAsObjectFiles(prefix, [suffix])	Save this DStream's contents as SequenceFiles of serialized Java objects. The file name at each batch interval is generated based on prefix and suffix:
         * "prefix-TIME_IN_MS[.suffix]".
         * Python API This is not available in the Python API.
         * saveAsHadoopFiles(prefix, [suffix])	Save this DStream's contents as Hadoop files. The file name at each batch interval is generated based on prefix and suffix: "prefix-TIME_IN_MS[.suffix]".
         * Python API This is not available in the Python API.
         * foreachRDD(func)	The most generic output operator that applies a function, func, to each RDD generated from the stream. This function should push the data in each RDD to an external system,
         * such as saving the RDD to files, or writing it over the network to a database. Note that the function func is executed in the driver process running the streaming application, and will usually
         * have RDD actions in it that will force the computation of the streaming RDDs.
         * */



        /**
         * Design Patterns for using foreachRDD
         * dstream.foreachRDD is a powerful primitive that allows data to be sent out to external systems. However,
         * it is important to understand how to use this primitive correctly and efficiently. Some of the common mistakes to avoid are as follows.
         *
         * Often writing data to external system requires creating a connection object (e.g. TCP connection to a remote server) and using it to send
         * data to a remote system. For this purpose, a developer may inadvertently try creating a connection object at the Spark driver, and then try
         * to use it in a Spark worker to save records in the RDDs. For example (in Scala),
         *
         * This amortizes the connection creation overheads over many records.
         *
         * Finally, this can be further optimized by reusing connection objects across multiple RDDs/batches. One can maintain a static pool of
         * connection objects than can be reused as RDDs of multiple batches are pushed to the external system, thus further reducing the overheads.
         *
         *
         * dstream.foreachRDD(rdd -> {
         *   rdd.foreachPartition(partitionOfRecords -> {
         *     // ConnectionPool is a static, lazily initialized pool of connections
         *     Connection connection = ConnectionPool.getConnection();
         *     while (partitionOfRecords.hasNext()) {
         *       connection.send(partitionOfRecords.next());
         *     }
         *     ConnectionPool.returnConnection(connection); // return to the pool for future reuse
         *   });
         * });
         * */

        //下面几点需要注意！！！
        /**
         * Note that the connections in the pool should be lazily created on demand and timed out if not used for a while. This achieves the most efficient sending of data to external systems.
         *
         * Other points to remember:
         * DStreams are executed lazily by the output operations, just like RDDs are lazily executed by RDD actions. Specifically, RDD actions inside the DStream output operations force the
         * processing of the received data. Hence, if your application does not have any output operation, or has output operations like dstream.foreachRDD() without any RDD action inside
         * them, then nothing will get executed. The system will simply receive the data and discard it.
         *
         * By default, output operations are executed one-at-a-time. And they are executed in the order they are defined in the application.
         * */




        /**
         * 下面的这些操作将在后面慢慢使用和学习中体现
         * */
        // UpdateStateByKey Operation
        //Transform Operation （转换操作）
        //Window Operations  （时间窗口操作）
        //Join Operations
        //Output Operations on DStreams
        //DataFrame and SQL Operations   （sql操作）
        //MLlib Operations  （机器学习操作）
        //Caching / Persistence
        //Checkpointing
        //Accumulators, Broadcast Variables, and Checkpoints
        //Deploying Applications
        //Monitoring Applications
        //Performance Tuning
        //Reducing the Batch Processing Times
        //Data Serialization
        //Setting the Right Batch Interval
        //Fault-tolerance Semantics (容错语义)



        //从kafka 中取数据  整和kafka

        //Creating a Direct Stream
        //Note that the namespace for the import includes the version, org.apache.spark.streaming.kafka010
        Map<String, Object> kafkaParams = new HashMap<>();
        kafkaParams.put("bootstrap.servers", "localhost:9092,anotherhost:9092");
        kafkaParams.put("key.deserializer", StringDeserializer.class);
        kafkaParams.put("value.deserializer", StringDeserializer.class);
        kafkaParams.put("group.id", "use_a_separate_group_id_for_each_stream");
        kafkaParams.put("auto.offset.reset", "latest");
        kafkaParams.put("enable.auto.commit", false);

        Collection<String> topics = Arrays.asList("topicA", "topicB");



        JavaInputDStream<ConsumerRecord<String, String>> stream =
                KafkaUtils.createDirectStream(
                        javaStreamingContext,
                        LocationStrategies.PreferConsistent(),
                        ConsumerStrategies.<String, String>Subscribe(topics, kafkaParams)
                );


        JavaPairDStream<String, String> stringStringJavaPairDStream = stream.mapToPair(record -> new Tuple2<>(record.key(), record.value()));
        stringStringJavaPairDStream.print();

        /**
         * For possible kafkaParams, see Kafka consumer config docs. If your Spark batch duration is larger than the default Kafka heartbeat
         * session timeout (30 seconds), increase heartbeat.interval.ms and session.timeout.ms appropriately. For batches larger than 5 minutes,
         * this will require changing group.max.session.timeout.ms on the broker. Note that the example sets enable.auto.commit to false, for discussion see Storing Offsets below.
         * */


        /**
         * LocationStrategies
         * The new Kafka consumer API will pre-fetch messages into buffers. Therefore it is important for performance reasons that the Spark integration keep cached consumers on executors (rather
         * than recreating them for each batch), and prefer to schedule partitions on the host locations that have the appropriate consumers.
         *
         * In most cases, you should use LocationStrategies.PreferConsistent as shown above. This will distribute partitions evenly across available executors. If your executors are on the same
         * hosts as your Kafka brokers, use PreferBrokers, which will prefer to schedule partitions on the Kafka leader for that partition. Finally, if you have a significant skew in load among
         * partitions, use PreferFixed. This allows you to specify an explicit mapping of partitions to hosts (any unspecified partitions will use a consistent location).
         *
         * The cache for consumers has a default maximum size of 64. If you expect to be handling more than (64 * number of executors) Kafka partitions, you can change this setting via spark.
         * streaming.kafka.consumer.cache.maxCapacity.
         *
         * If you would like to disable the caching for Kafka consumers, you can set spark.streaming.kafka.consumer.cache.enabled to false. Disabling the cache may be needed to workaround the
         * problem described in SPARK-19185. This property may be removed in later versions of Spark, once SPARK-19185 is resolved.
         *
         * The cache is keyed by topicpartition and group.id, so use a separate group.id for each call to createDirectStream.
         *
         * ConsumerStrategies
         * The new Kafka consumer API has a number of different ways to specify topics, some of which require considerable post-object-instantiation setup. ConsumerStrategies provides an abstraction
         * that allows Spark to obtain properly configured consumers even after restart from checkpoint.
         *
         * ConsumerStrategies.Subscribe, as shown above, allows you to subscribe to a fixed collection of topics. SubscribePattern allows you to use a regex to specify topics of interest. Note that
         * unlike the 0.8 integration, using Subscribe or SubscribePattern should respond to adding partitions during a running stream. Finally, Assign allows you to specify a fixed collection of
         * partitions. All three strategies have overloaded constructors that allow you to specify the starting offset for a particular partition.
         *
         * If you have specific consumer setup needs that are not met by the options above, ConsumerStrategy is a public class that you can extend.
         *
         * Creating an RDD
         * If you have a use case that is better suited to batch processing, you can create an RDD for a defined range of offsets.
         *
         *
         *
         * // Import dependencies and create kafka params as in Create Direct Stream above
         *
         * OffsetRange[] offsetRanges = {
         *   // topic, partition, inclusive starting offset, exclusive ending offset
         *   OffsetRange.create("test", 0, 0, 100),
         *   OffsetRange.create("test", 1, 0, 100)
         * };
         *
         * JavaRDD<ConsumerRecord<String, String>> rdd = KafkaUtils.createRDD(
         *   sparkContext,
         *   kafkaParams,
         *   offsetRanges,
         *   LocationStrategies.PreferConsistent()
         * );
         *
         * Note that you cannot use PreferBrokers, because without the stream there is not a driver-side consumer to automatically look up
         * broker metadata for you. Use PreferFixed with your own metadata lookups if necessary.
         * */

        stream.foreachRDD(rdd -> {
            OffsetRange[] offsetRanges = ((HasOffsetRanges) rdd.rdd()).offsetRanges();
            rdd.foreachPartition(consumerRecords -> {
                OffsetRange o = offsetRanges[TaskContext.get().partitionId()];
                System.out.println(
                        o.topic() + " " + o.partition() + " " + o.fromOffset() + " " + o.untilOffset());
            });
        });

        /**
         * Note that the typecast to HasOffsetRanges will only succeed if it is done in the first method called on the result of createDirectStream,
         * not later down a chain of methods. Be aware that the one-to-one mapping between RDD partition and Kafka partition does not remain after
         * any methods that shuffle or repartition, e.g. reduceByKey() or window().
         * */



        /**
         * Storing Offsets
         * Kafka delivery semantics in the case of failure depend on how and when offsets are stored. Spark output operations are at-least-once.
         * So if you want the equivalent of exactly-once semantics, you must either store offsets after an idempotent output, or store offsets
         * in an atomic transaction alongside output. With this integration, you have 3 options, in order of increasing reliability (and code complexity), for how to store offsets.
         *
         * Checkpoints
         * If you enable Spark checkpointing, offsets will be stored in the checkpoint. This is easy to enable, but there are drawbacks.
         * Your output operation must be idempotent, since you will get repeated outputs; transactions are not an option. Furthermore,
         * you cannot recover from a checkpoint if your application code has changed. For planned upgrades, you can mitigate this by
         * running the new code at the same time as the old code (since outputs need to be idempotent anyway, they should not clash).
         * But for unplanned failures that require code changes, you will lose data unless you have another way to identify known good starting offsets.
         *
         * Kafka itself
         * Kafka has an offset commit API that stores offsets in a special Kafka topic. By default, the new consumer will periodically auto-commit
         * offsets. This is almost certainly not what you want, because messages successfully polled by the consumer may not yet have resulted in a S
         * park output operation, resulting in undefined semantics. This is why the stream example above sets “enable.auto.commit” to false. However,
         * you can commit offsets to Kafka after you know your output has been stored, using the commitAsync API. The benefit as compared to checkpoints
         * is that Kafka is a durable store regardless of changes to your application code. However, Kafka is not transactional, so your outputs must still be idempotent.
         * */

        stream.foreachRDD(rdd -> {
            OffsetRange[] offsetRanges = ((HasOffsetRanges) rdd.rdd()).offsetRanges();

            // some time later, after outputs have completed
            ((CanCommitOffsets) stream.inputDStream()).commitAsync(offsetRanges);
        });


        /**
         * Your own data store
         * For data stores that support transactions, saving offsets in the same transaction as the results can keep the two in sync, even in failure situations.
         * If you’re careful about detecting repeated or skipped offset ranges, rolling back the transaction prevents duplicated or lost messages from affecting
         * results. This gives the equivalent of exactly-once semantics. It is also possible to use this tactic even for outputs that result from aggregations,
         * which are typically hard to make idempotent.
         * */


        // The details depend on your data store, but the general idea looks like this

        // begin from the the offsets committed to the database
        /*Map<TopicPartition, Long> fromOffsets = new HashMap<>();
        for (resultSet : selectOffsetsFromYourDatabase){
            fromOffsets.put(new TopicPartition(resultSet.string("topic"), resultSet.int("partition")), resultSet.long("offset"));
        }

        JavaInputDStream<ConsumerRecord<String, String>> streamss = KafkaUtils.createDirectStream(
            javaStreamingContext,
            LocationStrategies.PreferConsistent(),
            ConsumerStrategies.<String, String>Assign(fromOffsets.keySet(), kafkaParams, fromOffsets)
        );

        streamss.foreachRDD(rdd -> {
        OffsetRange[] offsetRanges = ((HasOffsetRanges) rdd.rdd()).offsetRanges();

        //Object results = yourCalculation(rdd);

        // begin your transaction

        // update results
        // update offsets where the end of existing offsets matches the beginning of this batch of offsets
        // assert that offsets were updated correctly

        // end your transaction
        });*/
        javaStreamingContext.start();
        javaStreamingContext.awaitTermination();


    }




}
